Репликация данных - хранение копиц данных на разных физических машинах соединенных в сети.

Репликация может быть нужна для следующих целей:
 - хранение данных в разных географических точках.
 - повышение доступности системы.
 - повышения пропускной способности системы, за счет работы разных инстансов.

# Виды репликации
## Master-Slave
В системе есть главная нодa - мастер, и ведомые ноды - слейвы. Читать можно из всех, но писать только в мастера.

Организация того, как происходит запись в слейвов, может быть _синхронная_ и _асинхронная_. В синхронном варианты мы ждем когда мастер раскидает все данные по всем слейвам, перед тем как вернуть положительный ответ клиенту. В асинхронном мы пишем в мастер, возвращаем ответ клиенту и в бэкграунде дописываем данные в слейвах. 

Также можно воспользоваться смешанных режимом, когда синхронном пишем в какую-то часть реплик и считаем, что этого достаточно, чтобы ответить клиенту об успешности операции. Можно выбирать разное количество этих реплик, например работать через кворум.

### Добавление нового узла
Добавления нового слейва может быть нетривиальным, но в общем случае выглядит так:
1. делаем снапшот базы и заливаем его на инстанс реплики.
2. разворачиваем снапшот.
3. дозапрашиваем журнал изменений.
4. применяем все изменения.
5. профит.

### Восстановление после отказа
#### Slave
У каждого слейва есть журнал, мы можем узнать на каком этапе слейв помер и актуализировать данные.

#### Master
Всё чуточку сложнее. Сначала нужно установить, что мастер действительно лежит. Ставим таймаут, который не будет давать сигналов о "мнимном сбое". Далее мы находим слейва с последними обновлениями от мастера и делаем его мастером. Сообщаем всем, что у нас сменился мастер. Если отмирает старый мастер и пытается занять свое место, то делаем его слейвом.

Тут остается проблема, что делать с данными, которые были на мастере и не успели попасть на реплики. Особенно актуально при асинхронной репликации.

### Журнал репликации
#### Операторная репликация
Мастер отправляет все запрсосы слейвам как будто прокси-сервер. Все слейвы самостоятельно разбирают запросы и строят план выполнения. Такой подход сулит неконсистентность данных, особенно если в запросе есть вызовы каких-либо встроенный функций (rand(), now() и тд).

#### Перенос журнала упреждающей записи (WAL)
Журнал, в который дописывается информация на низком уровне. Информация пищется в байтах и содержит в себе данные о результате выполнения функций, о том как менялись даннные в дисковых блоках. Способ использует PostgreSQL и Oracle. Из минусов можно отметить, что не всегда есть возможность работать с разными версиями баз.

#### Логическая (построчная) репликация
В журнал пишутся удобного формата строки - вызовы апи.

#### Триггерная
Ручное управление переносами данных, срабатывает на триггер и выполняет какой-то код.

[разбор всяких разных репликаций БД](https://habr.com/ru/post/514500/)


### Масштабируемость для снижения времени отклика
Такая масштабируемость через репликации (поднятие инстансов на серверах в разных гео-точках) должна работать в асинхронном режиме, иначе будет одна большая точка отказа и большие задержки на запись.

Конечно, будут задержки и в асинхронном режиме, но задержки для реплик - реплики будут отдавать неактуальные данне. _Конечная согласованность_ говорит о том, что если прекратить запись, то через какое-то время мы получим согласованные данные и на слейвах и на мастере. Время это не уточняется. Время нужное для достижения конечной согласованности - _задержка репликации_.

Тут могут возникать разные проблемы с чтением несогласованных данных и вот как можно некоторые из них закрыть:
- Читаем свои собственные данные всегда из мастер реплики, а данные которые не пренадлежат пользователю из ближайшей. (работает если у пользователя не очень много данных)
- "отслеживать время последнего обновления и в течение одной минуты после этого читать все с ведущего узла." - тут я не очень понял о чем речь. Время обновления чего? ресурса? страницы просто? Выглядит как слишком сложно.
- Не читать данные с реплик, в которых слишком большая задержка репликации.
- Следить в целом за последней операцией записи от клиента. Скидывать в кэш (так как могут быть клиенты с разных устройств) или записывать на стороне клиента (тут нужно поддерживать нормальную синхронизацию).
- Привязывать пользователей к определенным репликам, реализуя _монотонное чтение_. (Тут же опять всякие разные сложности с определением этих реплик)

## Master-Master
Полезно, когда у нас несколько ЦОДов. На уровне одного ЦОДа у нас также система с master-slave, а на глобальном уровне у нас между собой мастер одного цода реплицирует данные на мастер другого ЦОДа.

- Возрастает производтельность записи, так как не надо гонять все через один ЦОД, каждый сервис использует локальный ЦОД.
- Один ЦОД может подменить другой во время сбоя - повышаем устойчивость.

Также можно столкнуться с другими сложностями - одновременная модификация данных и другие подводные камни присущие самой СУБД.

Методы разрешения конфликтов данных:
- присваивать какие-либо метки каждому запросу и выбирать "наибольший", если это например число или наипоздний, если это таймстемп (Last Write Wins).
- присвоить номер реплике и записывать данные с определенной, читай проградуировать реплики по значимости.
- сложить данные, как сиамсиких близнецов.
- сообщать о конфликтах пользователю и просить его их разрешить.
- автоматические алогритмы разрешения конфликтов с использованием специальных структур данных. Этот метод работает не очень внутри СУБД как гооворит Клеппман

### Топологии master-master
На рисунке представлены топологии,  в которые могут объединяться системы мастер-мастер. 
[[топология мастер-мастер.png]]

Версии не "каждый с каждым" могут иметь точки отказа, но "каждый с каждым" может нарушать очередность записи. Запросы приходят на реплики в разное время.

## Slave-Slave
В системах где отсутствует мастер действуют другие правила. Примером может служить база данных DynamoDB.

В некоторых реализациях запись от клиента может разлетаться сразу во все реплики.

Механизмы для борьбы с неконсистентностью:
 - разрешение конфликтов при чтении. Использование _сильной консистентности_. Например в DynamoDB это указывается параметром при запросах. Используя сильную консистентность влечет за собой использование больших ресурсов и времени.
 - процесс противодействия энтропии. В бэкграунде база данных сама ищет такие конфликты и разрешает их.
 - запись и чтение по кворуму.
 
 _Тут еще не дочитал у Клеппмана глава 5.4_
 